INFO:training log:hparams...
INFO:training log:d_model:6 - dff:24 - positional encoding: 50 - learning rate: <train.loss_functions.CustomSchedule object at 0x139ba8850>
INFO:training log:Transformer with one head and one layer and no layer norm...
INFO:training log:Epoch 1/10
INFO:training log:train loss: 14.156012535095215 - val loss: 12.668582916259766
INFO:training log:Time taken for 1 epoch: 3.149517059326172 secs

INFO:training log:Epoch 2/10
INFO:training log:train loss: 10.619734764099121 - val loss: 8.224801063537598
INFO:training log:Time taken for 1 epoch: 2.8949050903320312 secs

INFO:training log:Epoch 3/10
INFO:training log:train loss: 6.206406593322754 - val loss: 4.312960147857666
INFO:training log:Time taken for 1 epoch: 2.8635520935058594 secs

INFO:training log:Epoch 4/10
INFO:training log:train loss: 3.0809357166290283 - val loss: 2.1130635738372803
INFO:training log:Time taken for 1 epoch: 2.8396782875061035 secs

INFO:training log:Epoch 5/10
INFO:training log:train loss: 1.622061848640442 - val loss: 1.3590283393859863
INFO:training log:Time taken for 1 epoch: 2.859891891479492 secs

INFO:training log:Epoch 6/10
INFO:training log:train loss: 1.2569724321365356 - val loss: 1.243157982826233
INFO:training log:Time taken for 1 epoch: 2.974151849746704 secs

INFO:training log:Epoch 7/10
INFO:training log:train loss: 1.165380597114563 - val loss: 1.1236459016799927
INFO:training log:Time taken for 1 epoch: 2.818204164505005 secs

INFO:training log:Epoch 8/10
INFO:training log:train loss: 0.9236050248146057 - val loss: 0.7517663836479187
INFO:training log:Time taken for 1 epoch: 2.9206788539886475 secs

INFO:training log:Epoch 9/10
INFO:training log:train loss: 0.7031265497207642 - val loss: 0.6807035803794861
INFO:training log:Time taken for 1 epoch: 3.364074945449829 secs

INFO:training log:Epoch 10/10
INFO:training log:train loss: 0.6513491272926331 - val loss: 0.6466848850250244
INFO:training log:Time taken for 1 epoch: 3.4525198936462402 secs

INFO:training log:total training time for 10 epochs:30.140645265579224
INFO:training log:saving loss and metrics information...
INFO:training log:training of a classic Transformer for a time-series dataset done...
INFO:training log:>>>-------------------------------------------------------------------------------------------------------------------------------------------------------------<<<
INFO:training log:test loss at the end of training: 0.6282556056976318
INFO:training log:Latest checkpoint restored!!
INFO:training log:<--------------------------------------------------------------------------------------->
INFO:training log:starting SMC algo on trained Transformer...
INFO:training log:SMC params: sigma_k_q_v_z: 0.05 - sigma_obs:0.5
INFO:training log:SMC for number of particles: 10
INFO:training log:normal loss on first batch...0.6665719151496887
INFO:training log:smc train loss at batch 1: 7.735884189605713
INFO:training log:smc train loss at batch 2: 7.377624034881592
INFO:training log:smc train loss at batch 3: 7.732456684112549
INFO:training log:smc train loss at batch 4: 6.950590133666992
INFO:training log:smc train loss at batch 5: 8.020170211791992
INFO:training log:smc train loss at batch 6: 7.5240044593811035
INFO:training log:smc train loss at batch 7: 7.848677635192871
INFO:training log:smc train loss at batch 8: 7.960679054260254
INFO:training log:smc train loss at batch 9: 6.439540386199951
INFO:training log:smc train loss at batch 10: 7.261702060699463
INFO:training log:smc train loss at batch 11: 7.036149978637695
INFO:training log:smc train loss at batch 12: 7.988802909851074
INFO:training log:smc train loss at batch 13: 7.759310722351074
INFO:training log:smc train loss at batch 14: 7.612543106079102
INFO:training log:smc train loss at batch 15: 7.784826278686523
INFO:training log:smc train loss at batch 16: 8.275829315185547
INFO:training log:smc train loss at batch 17: 7.344217300415039
INFO:training log:smc train loss at batch 18: 7.049929141998291
INFO:training log:smc train loss at batch 19: 7.913421630859375
INFO:training log:smc train loss at batch 20: 7.414401054382324
INFO:training log:smc train loss at batch 21: 8.42990493774414
INFO:training log:smc train loss at batch 22: 8.01891040802002
INFO:training log:smc train loss at batch 23: 8.644942283630371
INFO:training log:smc train loss at batch 24: 7.988813400268555
INFO:training log:smc train loss at batch 25: 8.130931854248047
INFO:training log:smc train loss at batch 26: 6.988883018493652
INFO:training log:smc train loss at batch 27: 7.844448089599609
INFO:training log:smc train loss at batch 28: 8.12255859375
INFO:training log:smc train loss at batch 29: 7.648180961608887
INFO:training log:smc train loss at batch 30: 7.55271577835083
INFO:training log:smc train loss at batch 31: 8.135592460632324
INFO:training log:smc train loss at batch 32: 8.544134140014648
INFO:training log:smc train loss at batch 33: 7.6808271408081055
INFO:training log:smc train loss at batch 34: 7.955452919006348
INFO:training log:smc train loss at batch 35: 7.465759754180908
INFO:training log:smc train loss at batch 36: 7.667385578155518
INFO:training log:smc train loss at batch 37: 8.649468421936035
INFO:training log:smc train loss at batch 38: 7.184140682220459
INFO:training log:smc train loss at batch 39: 6.890298366546631
INFO:training log:smc train loss at batch 40: 7.065135955810547
INFO:training log:smc train loss at batch 41: 7.086216926574707
INFO:training log:smc train loss at batch 42: 7.9573259353637695
INFO:training log:smc train loss at batch 43: 7.47064208984375
INFO:training log:smc train loss at batch 44: 8.547431945800781
INFO:training log:smc train loss at batch 45: 7.637569427490234
INFO:training log:smc train loss at batch 46: 6.76511287689209
INFO:training log:smc train loss at batch 47: 7.425182342529297
INFO:training log:smc train loss at batch 48: 7.1469011306762695
INFO:training log:smc train loss at batch 49: 7.256006717681885
INFO:training log:smc train loss at batch 50: 7.1385908126831055
INFO:training log:smc train loss at batch 51: 7.675671577453613
INFO:training log:smc train loss at batch 52: 8.941312789916992
INFO:training log:smc train loss at batch 53: 7.945034027099609
INFO:training log:smc train loss at batch 54: 7.145914077758789
INFO:training log:<----------------------------------------------------->
INFO:training log:smc val loss at batch 1: 7.287988662719727
INFO:training log:smc train loss at batch 1: 7.145914077758789
INFO:training log:smc val loss at batch 2: 8.044387817382812
INFO:training log:smc train loss at batch 2: 7.145914077758789
INFO:training log:smc val loss at batch 3: 8.033807754516602
INFO:training log:smc train loss at batch 3: 7.145914077758789
INFO:training log:smc val loss at batch 4: 7.522278785705566
INFO:training log:smc train loss at batch 4: 7.145914077758789
INFO:training log:smc val loss at batch 5: 7.798418998718262
INFO:training log:smc train loss at batch 5: 7.145914077758789
INFO:training log:smc val loss at batch 6: 8.051431655883789
INFO:training log:smc train loss at batch 6: 7.145914077758789
INFO:training log:smc val loss at batch 7: 7.8425984382629395
INFO:training log:smc train loss at batch 7: 7.145914077758789
INFO:training log:smc val loss at batch 8: 7.424072265625
INFO:training log:smc train loss at batch 8: 7.145914077758789
INFO:training log:smc val loss at batch 9: 7.917868614196777
INFO:training log:smc train loss at batch 9: 7.145914077758789
INFO:training log:smc val loss at batch 10: 7.549919605255127
INFO:training log:smc train loss at batch 10: 7.145914077758789
INFO:training log:smc val loss at batch 11: 7.310936450958252
INFO:training log:smc train loss at batch 11: 7.145914077758789
INFO:training log:<--------------------------------------------------------------------------->
INFO:training log:<----------------------------------------------------------------------------------------------------->
INFO:training log:SMC params: sigma_k_q_v_z: 0.1 - sigma_obs:0.7
INFO:training log:SMC for number of particles: 10
INFO:training log:normal loss on first batch...1.3652675151824951
INFO:training log:smc train loss at batch 1: 7.278846740722656
INFO:training log:smc train loss at batch 2: 7.201346397399902
INFO:training log:smc train loss at batch 3: 6.86521053314209
INFO:training log:smc train loss at batch 4: 7.380634307861328
INFO:training log:smc train loss at batch 5: 6.708260536193848
INFO:training log:smc train loss at batch 6: 7.675559043884277
INFO:training log:smc train loss at batch 7: 7.299239158630371
INFO:training log:smc train loss at batch 8: 7.236641883850098
INFO:training log:smc train loss at batch 9: 7.651741981506348
INFO:training log:smc train loss at batch 10: 7.485712051391602
INFO:training log:smc train loss at batch 11: 7.750615119934082
INFO:training log:smc train loss at batch 12: 7.551877498626709
INFO:training log:smc train loss at batch 13: 7.254047870635986
INFO:training log:smc train loss at batch 14: 7.53791618347168
INFO:training log:smc train loss at batch 15: 7.398319721221924
INFO:training log:smc train loss at batch 16: 8.1083984375
INFO:training log:smc train loss at batch 17: 7.494677543640137
INFO:training log:smc train loss at batch 18: 7.697902679443359
INFO:training log:smc train loss at batch 19: 7.4742231369018555
INFO:training log:smc train loss at batch 20: 7.892463207244873
INFO:training log:smc train loss at batch 21: 7.257122039794922
INFO:training log:smc train loss at batch 22: 6.871507167816162
INFO:training log:smc train loss at batch 23: 7.7589111328125
INFO:training log:smc train loss at batch 24: 7.910711288452148
INFO:training log:smc train loss at batch 25: 7.879812717437744
INFO:training log:smc train loss at batch 26: 8.453047752380371
INFO:training log:smc train loss at batch 27: 7.700649261474609
INFO:training log:smc train loss at batch 28: 7.499580383300781
INFO:training log:smc train loss at batch 29: 7.220791339874268
INFO:training log:smc train loss at batch 30: 7.266382694244385
INFO:training log:smc train loss at batch 31: 7.945878982543945
INFO:training log:smc train loss at batch 32: 7.3080902099609375
INFO:training log:smc train loss at batch 33: 7.587006092071533
INFO:training log:smc train loss at batch 34: 8.262443542480469
INFO:training log:smc train loss at batch 35: 7.823604583740234
INFO:training log:smc train loss at batch 36: 7.39009952545166
INFO:training log:smc train loss at batch 37: 7.781273365020752
INFO:training log:smc train loss at batch 38: 6.867440223693848
INFO:training log:smc train loss at batch 39: 7.580871105194092
INFO:training log:smc train loss at batch 40: 7.263091564178467
INFO:training log:smc train loss at batch 41: 6.869146347045898
INFO:training log:smc train loss at batch 42: 7.420938968658447
INFO:training log:smc train loss at batch 43: 7.446297645568848
INFO:training log:smc train loss at batch 44: 7.8942952156066895
INFO:training log:smc train loss at batch 45: 7.516597747802734
INFO:training log:smc train loss at batch 46: 7.529940605163574
INFO:training log:smc train loss at batch 47: 7.685934543609619
INFO:training log:smc train loss at batch 48: 7.286457061767578
INFO:training log:smc train loss at batch 49: 7.734896659851074
INFO:training log:smc train loss at batch 50: 7.2191853523254395
INFO:training log:smc train loss at batch 51: 7.246165752410889
INFO:training log:smc train loss at batch 52: 7.432590007781982
INFO:training log:smc train loss at batch 53: 7.486410140991211
INFO:training log:smc train loss at batch 54: 7.898237228393555
INFO:training log:<----------------------------------------------------->
INFO:training log:smc val loss at batch 1: 7.076263904571533
INFO:training log:smc train loss at batch 1: 7.898237228393555
INFO:training log:smc val loss at batch 2: 7.855216026306152
INFO:training log:smc train loss at batch 2: 7.898237228393555
INFO:training log:smc val loss at batch 3: 7.847581386566162
INFO:training log:smc train loss at batch 3: 7.898237228393555
INFO:training log:smc val loss at batch 4: 7.358243942260742
INFO:training log:smc train loss at batch 4: 7.898237228393555
INFO:training log:smc val loss at batch 5: 7.598639965057373
INFO:training log:smc train loss at batch 5: 7.898237228393555
INFO:training log:smc val loss at batch 6: 7.894990921020508
INFO:training log:smc train loss at batch 6: 7.898237228393555
INFO:training log:smc val loss at batch 7: 7.6560516357421875
INFO:training log:smc train loss at batch 7: 7.898237228393555
INFO:training log:smc val loss at batch 8: 7.2679667472839355
INFO:training log:smc train loss at batch 8: 7.898237228393555
INFO:training log:smc val loss at batch 9: 7.740372180938721
INFO:training log:smc train loss at batch 9: 7.898237228393555
INFO:training log:smc val loss at batch 10: 7.425555229187012
INFO:training log:smc train loss at batch 10: 7.898237228393555
INFO:training log:smc val loss at batch 11: 7.174646377563477
INFO:training log:smc train loss at batch 11: 7.898237228393555
INFO:training log:<--------------------------------------------------------------------------->
INFO:training log:<----------------------------------------------------------------------------------------------------->
INFO:training log:SMC params: sigma_k_q_v_z: 0.5 - sigma_obs:0.9
INFO:training log:SMC for number of particles: 10
INFO:training log:normal loss on first batch...1.3829400539398193
INFO:training log:smc train loss at batch 1: 5.756332874298096
INFO:training log:smc train loss at batch 2: 6.016444206237793
INFO:training log:smc train loss at batch 3: 5.818120002746582
INFO:training log:smc train loss at batch 4: 5.950153827667236
INFO:training log:smc train loss at batch 5: 5.7596893310546875
INFO:training log:smc train loss at batch 6: 5.162716865539551
INFO:training log:smc train loss at batch 7: 6.086738109588623
INFO:training log:smc train loss at batch 8: 6.021755695343018
INFO:training log:smc train loss at batch 9: 6.042973518371582
INFO:training log:smc train loss at batch 10: 6.016175270080566
INFO:training log:smc train loss at batch 11: 6.008642196655273
INFO:training log:smc train loss at batch 12: 5.855458736419678
INFO:training log:smc train loss at batch 13: 6.267029762268066
INFO:training log:smc train loss at batch 14: 6.402239799499512
INFO:training log:smc train loss at batch 15: 6.135472297668457
INFO:training log:smc train loss at batch 16: 6.310635566711426
INFO:training log:smc train loss at batch 17: 5.927855014801025
INFO:training log:smc train loss at batch 18: 6.383876800537109
INFO:training log:smc train loss at batch 19: 5.67191219329834
INFO:training log:smc train loss at batch 20: 5.867377281188965
INFO:training log:smc train loss at batch 21: 6.288493633270264
INFO:training log:smc train loss at batch 22: 6.113439559936523
INFO:training log:smc train loss at batch 23: 5.828887939453125
INFO:training log:smc train loss at batch 24: 6.461097717285156
INFO:training log:smc train loss at batch 25: 6.184514999389648
INFO:training log:smc train loss at batch 26: 6.081697940826416
INFO:training log:smc train loss at batch 27: 6.029429912567139
INFO:training log:smc train loss at batch 28: 6.491909980773926
INFO:training log:smc train loss at batch 29: 5.471920967102051
INFO:training log:smc train loss at batch 30: 5.983930587768555
INFO:training log:smc train loss at batch 31: 5.088181972503662
INFO:training log:smc train loss at batch 32: 6.132897853851318
INFO:training log:smc train loss at batch 33: 6.57227087020874
INFO:training log:smc train loss at batch 34: 6.058444976806641
INFO:training log:smc train loss at batch 35: 5.632386684417725
INFO:training log:smc train loss at batch 36: 5.627878189086914
INFO:training log:smc train loss at batch 37: 6.544031143188477
INFO:training log:smc train loss at batch 38: 5.933943748474121
INFO:training log:smc train loss at batch 39: 5.602386951446533
INFO:training log:smc train loss at batch 40: 6.425085067749023
INFO:training log:smc train loss at batch 41: 6.026939392089844
INFO:training log:smc train loss at batch 42: 6.599068641662598
INFO:training log:smc train loss at batch 43: 5.699542045593262
INFO:training log:smc train loss at batch 44: 5.708579063415527
INFO:training log:smc train loss at batch 45: 5.843633651733398
INFO:training log:smc train loss at batch 46: 5.7780866622924805
INFO:training log:smc train loss at batch 47: 6.119129180908203
INFO:training log:smc train loss at batch 48: 6.387920379638672
INFO:training log:smc train loss at batch 49: 6.301856517791748
INFO:training log:smc train loss at batch 50: 6.2328033447265625
INFO:training log:smc train loss at batch 51: 6.00004768371582
INFO:training log:smc train loss at batch 52: 5.525019645690918
INFO:training log:smc train loss at batch 53: 6.253724098205566
INFO:training log:smc train loss at batch 54: 6.475894451141357
INFO:training log:<----------------------------------------------------->
INFO:training log:smc val loss at batch 1: 5.761349201202393
INFO:training log:smc train loss at batch 1: 6.475894451141357
INFO:training log:smc val loss at batch 2: 6.138659477233887
INFO:training log:smc train loss at batch 2: 6.475894451141357
INFO:training log:smc val loss at batch 3: 5.849352836608887
INFO:training log:smc train loss at batch 3: 6.475894451141357
INFO:training log:smc val loss at batch 4: 5.904626369476318
INFO:training log:smc train loss at batch 4: 6.475894451141357
INFO:training log:smc val loss at batch 5: 5.969732284545898
INFO:training log:smc train loss at batch 5: 6.475894451141357
INFO:training log:smc val loss at batch 6: 6.421921253204346
INFO:training log:smc train loss at batch 6: 6.475894451141357
INFO:training log:smc val loss at batch 7: 6.069312572479248
INFO:training log:smc train loss at batch 7: 6.475894451141357
INFO:training log:smc val loss at batch 8: 5.575506687164307
INFO:training log:smc train loss at batch 8: 6.475894451141357
INFO:training log:smc val loss at batch 9: 6.202830791473389
INFO:training log:smc train loss at batch 9: 6.475894451141357
INFO:training log:smc val loss at batch 10: 6.047850608825684
INFO:training log:smc train loss at batch 10: 6.475894451141357
INFO:training log:smc val loss at batch 11: 5.750589370727539
INFO:training log:smc train loss at batch 11: 6.475894451141357
INFO:training log:<--------------------------------------------------------------------------->
INFO:training log:<----------------------------------------------------------------------------------------------------->
